name: "[Dagger] CI"

on:
  push:
    paths:
      - "src/leibniz.*"
      - "src/rounds.txt"
      - "dagger-poc/languages.py"
      - "dagger-poc/benchmark.py"
      - "dagger-poc/build_images.py"
      - "dagger-poc/scmeta.py"
    branches:
      - master
  pull_request:
    paths:
      - "src/leibniz.*"
      - "dagger-poc/**"
  # Handle /dagger-bench comments on PRs
  issue_comment:
    types: [created]
  workflow_dispatch:
    inputs:
      languages:
        description: 'Languages to build/benchmark (space-separated, e.g., "rust go python"). Leave empty for all, "all" for everything.'
        required: false
        type: string
      build_only:
        description: "Only build images, skip benchmarks"
        required: false
        type: boolean
        default: false
      benchmark_only:
        description: "Only run benchmarks (assumes images exist)"
        required: false
        type: boolean
        default: false
      quick_test:
        description: "Quick test mode (10000 iterations instead of 1 billion)"
        required: false
        type: boolean
        default: false
      skip_cache:
        description: "Skip cache and run fresh benchmarks"
        required: false
        type: boolean
        default: false
      dry_run:
        description: "Dry run (don't push images to registry)"
        required: false
        type: boolean
        default: false

env:
  REGISTRY: ghcr.io/${{ github.repository }}
  DAGGER_VERSION: "0.19.8"

jobs:
  # ==========================================================================
  # PR Comment Handler (/dagger-bench command)
  # ==========================================================================
  parse-comment:
    runs-on: ubicloud-standard-2
    if: github.event_name == 'issue_comment' && github.event.issue.pull_request && startsWith(github.event.comment.body, '/dagger-bench')
    outputs:
      languages: ${{ steps.parse.outputs.languages }}
      should_run: ${{ steps.parse.outputs.should_run }}
      show_help: ${{ steps.parse.outputs.show_help }}
      pr_ref: ${{ steps.get-pr.outputs.ref }}
      pr_sha: ${{ steps.get-pr.outputs.sha }}
      pr_repo: ${{ steps.get-pr.outputs.repo }}
      pr_number: ${{ steps.get-pr.outputs.pr_number }}
    steps:
      - name: Check if comment author is trusted
        id: check-permission
        uses: actions/github-script@v7
        with:
          script: |
            const association = context.payload.comment.author_association;
            const trustedAssociations = ['OWNER', 'MEMBER', 'COLLABORATOR', 'CONTRIBUTOR'];
            const authorized = trustedAssociations.includes(association);
            console.log(`Comment author: ${context.payload.comment.user.login}`);
            console.log(`Author association: ${association}`);
            console.log(`Authorized: ${authorized}`);
            core.setOutput('authorized', authorized ? 'true' : 'false');

      - name: Parse /dagger-bench command
        id: parse
        if: steps.check-permission.outputs.authorized == 'true'
        run: |
          COMMENT="${{ github.event.comment.body }}"
          LANGUAGES=$(echo "$COMMENT" | sed -n 's/^\/dagger-bench[[:space:]]*//p' | tr -s ' ' | xargs)

          if [ -z "$LANGUAGES" ]; then
            echo "No languages specified, will show help"
            echo "should_run=false" >> $GITHUB_OUTPUT
            echo "show_help=true" >> $GITHUB_OUTPUT
          elif [ "$LANGUAGES" = "help" ]; then
            echo "Help requested"
            echo "should_run=false" >> $GITHUB_OUTPUT
            echo "show_help=true" >> $GITHUB_OUTPUT
          else
            echo "Languages to benchmark: $LANGUAGES"
            echo "languages=$LANGUAGES" >> $GITHUB_OUTPUT
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "show_help=false" >> $GITHUB_OUTPUT
          fi

      - name: Get PR info
        id: get-pr
        if: steps.check-permission.outputs.authorized == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const pr = await github.rest.pulls.get({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.issue.number
            });
            core.setOutput('ref', pr.data.head.ref);
            core.setOutput('sha', pr.data.head.sha);
            core.setOutput('repo', pr.data.head.repo.full_name);
            core.setOutput('pr_number', context.issue.number);

      - name: Show help
        if: steps.parse.outputs.show_help == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const helpText = [
              '## /dagger-bench Command Help',
              '',
              '**Usage:**',
              '- `/dagger-bench <language> [language2] ...` - Benchmark specific languages',
              '- `/dagger-bench help` - Show this help message',
              '',
              '**Examples:**',
              '- `/dagger-bench rust go c` - Benchmark Rust, Go, and C',
              '- `/dagger-bench julia` - Benchmark Julia',
              '',
              '**Note:** Use target names from `dagger-poc/languages.py` (e.g., `rust`, `cpython`, `nodejs`).'
            ].join('\n');

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: helpText
            });

      - name: Add reaction to comment
        if: steps.parse.outputs.should_run == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.reactions.createForIssueComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: context.payload.comment.id,
              content: 'rocket'
            });

  # Run benchmarks from /dagger-bench command (simplified, single job)
  bench-comment:
    needs: parse-comment
    if: needs.parse-comment.outputs.should_run == 'true'
    runs-on: ubicloud-standard-4
    permissions:
      checks: write
      pull-requests: write
      contents: read
      packages: read
    steps:
      - name: Create check run
        id: check
        uses: actions/github-script@v7
        with:
          script: |
            const check = await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: '[Dagger] Benchmark: ${{ needs.parse-comment.outputs.languages }}',
              head_sha: '${{ needs.parse-comment.outputs.pr_sha }}',
              status: 'in_progress',
              output: {
                title: 'Running Dagger benchmarks',
                summary: 'Benchmarking: `${{ needs.parse-comment.outputs.languages }}`'
              }
            });
            core.setOutput('check_id', check.data.id);

      - uses: actions/checkout@v4
        with:
          repository: ${{ needs.parse-comment.outputs.pr_repo }}
          ref: ${{ needs.parse-comment.outputs.pr_sha }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Install Dagger
        run: |
          curl -fsSL https://dl.dagger.io/dagger/install.sh | DAGGER_VERSION=${{ env.DAGGER_VERSION }} BIN_DIR=/usr/local/bin sudo -E sh

      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Run benchmarks
        id: bench
        working-directory: dagger-poc
        env:
          REGISTRY: ${{ env.REGISTRY }}
        run: |
          uv sync --quiet
          LANGUAGES="${{ needs.parse-comment.outputs.languages }}"
          echo "Running Dagger benchmarks for: $LANGUAGES"
          FAILED=""
          for lang in $LANGUAGES; do
            echo "::group::Benchmarking $lang"
            if uv run dagger run python benchmark.py $lang; then
              echo "Success: $lang"
            else
              FAILED="$FAILED $lang"
              echo "::error::Benchmark for $lang failed"
            fi
            echo "::endgroup::"
          done
          if [ -n "$FAILED" ]; then
            echo "failed=$FAILED" >> $GITHUB_OUTPUT
          fi

      - name: Comment results on PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            const languages = '${{ needs.parse-comment.outputs.languages }}';
            const failed = '${{ steps.bench.outputs.failed }}'.trim();

            let comment = '## Dagger Benchmark Results\n\n';
            comment += `**Languages tested:** \`${languages}\`\n`;
            comment += `**Commit:** \`${{ needs.parse-comment.outputs.pr_sha }}\`\n\n`;

            if (failed) {
              comment += `> **Warning:** Some benchmarks failed: \`${failed}\`\n\n`;
            }

            const results = [];
            const resultsDir = 'results';
            try {
              const files = fs.readdirSync(resultsDir).filter(f => f.endsWith('.json'));
              for (const file of files) {
                const data = JSON.parse(fs.readFileSync(path.join(resultsDir, file), 'utf8'));
                results.push({
                  language: data.Language,
                  min: parseFloat(data.Min) * 1000,
                  median: parseFloat(data.Median) * 1000,
                  max: parseFloat(data.Max) * 1000,
                  accuracy: data.Accuracy ? data.Accuracy.toFixed(2) : 'N/A'
                });
              }

              if (results.length > 0) {
                results.sort((a, b) => a.median - b.median);
                comment += '| Language | Min | Median | Max | Accuracy |\n';
                comment += '|:---------|----:|-------:|----:|:--------:|\n';
                for (const r of results) {
                  comment += `| ${r.language} | ${r.min.toFixed(1)} ms | ${r.median.toFixed(1)} ms | ${r.max.toFixed(1)} ms | ${r.accuracy} |\n`;
                }
              }
            } catch (e) {
              comment += `_Could not parse results: ${e.message}_\n`;
            }

            comment += `\n[View workflow run](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})`;

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: ${{ needs.parse-comment.outputs.pr_number }},
              body: comment
            });

      - name: Update check run
        if: always() && steps.check.outputs.check_id
        uses: actions/github-script@v7
        with:
          script: |
            const failed = '${{ steps.bench.outputs.failed }}'.trim();
            const conclusion = failed ? 'failure' : 'success';
            await github.rest.checks.update({
              owner: context.repo.owner,
              repo: context.repo.repo,
              check_run_id: ${{ steps.check.outputs.check_id }},
              status: 'completed',
              conclusion: conclusion,
              output: {
                title: failed ? `Failed: ${failed}` : 'Benchmarks completed',
                summary: 'See workflow run for details'
              }
            });

  # ==========================================================================
  # Main CI Pipeline (push, PR, workflow_dispatch)
  # ==========================================================================

  # Determine what to build and benchmark
  prepare:
    runs-on: ubicloud-standard-2
    if: github.event_name != 'issue_comment'
    permissions:
      contents: read
      packages: read
    outputs:
      build_matrix: ${{ steps.detect.outputs.build_matrix }}
      bench_matrix: ${{ steps.detect.outputs.bench_matrix }}
      has_builds: ${{ steps.detect.outputs.has_builds }}
      has_benchmarks: ${{ steps.detect.outputs.has_benchmarks }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Log in to GHCR (for registry checks)
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Detect what needs building/benchmarking
        id: detect
        working-directory: dagger-poc
        env:
          REGISTRY: ${{ env.REGISTRY }}
        run: |
          uv sync --quiet

          # Helper functions
          get_all_langs() {
            uv run python -c "from languages import LANGUAGES; print(' '.join(LANGUAGES.keys()))"
          }

          get_all_bases() {
            uv run python -c "from languages import get_base_languages; print(' '.join(sorted(get_base_languages().keys())))"
          }

          targets_to_bases() {
            uv run python -c "from languages import resolve_targets_to_bases; bases = resolve_targets_to_bases('$1'.split()); print(' '.join(sorted(bases.keys())))"
          }

          # Determine which languages to process
          if [ -n "${{ inputs.languages }}" ]; then
            if [ "${{ inputs.languages }}" = "all" ]; then
              LANGS=$(get_all_langs)
            else
              LANGS="${{ inputs.languages }}"
            fi
          else
            LANGS=$(get_all_langs)
          fi

          # Determine images to build
          if [ "${{ inputs.benchmark_only }}" = "true" ]; then
            BUILD_LANGS=""
          elif [ "${{ github.event_name }}" = "push" ]; then
            # Check if build_images.py changed (rebuild all)
            if git diff --name-only HEAD~1 HEAD | grep -q "dagger-poc/build_images.py"; then
              echo "build_images.py changed - rebuilding all base images"
              BUILD_LANGS=$(get_all_bases)
            else
              # Detect changed + missing images
              BUILD_LANGS=$(uv run python detect_changes.py --check-registry)
            fi
          else
            # For PRs and dispatch, check registry for missing images
            BUILD_LANGS=$(targets_to_bases "$LANGS")
            # Filter to only missing images
            MISSING=""
            for base in $BUILD_LANGS; do
              if ! uv run python check_images.py $base --quiet 2>/dev/null; then
                MISSING="$MISSING $base"
              fi
            done
            BUILD_LANGS=$(echo "$MISSING" | xargs)
          fi

          # Determine benchmarks to run
          if [ "${{ inputs.build_only }}" = "true" ]; then
            BENCH_LANGS=""
          else
            BENCH_LANGS="$LANGS"
          fi

          # Output build matrix
          BUILD_LANGS=$(echo "$BUILD_LANGS" | xargs)
          if [ -z "$BUILD_LANGS" ]; then
            echo "No images to build"
            echo "has_builds=false" >> $GITHUB_OUTPUT
            echo "build_matrix={\"language\":[]}" >> $GITHUB_OUTPUT
          else
            echo "Images to build: $BUILD_LANGS"
            echo "has_builds=true" >> $GITHUB_OUTPUT
            JSON_ARRAY=$(echo "$BUILD_LANGS" | tr ' ' '\n' | jq -R . | jq -s -c .)
            echo "build_matrix={\"language\":$JSON_ARRAY}" >> $GITHUB_OUTPUT
          fi

          # Output benchmark matrix
          BENCH_LANGS=$(echo "$BENCH_LANGS" | xargs)
          if [ -z "$BENCH_LANGS" ]; then
            echo "No benchmarks to run"
            echo "has_benchmarks=false" >> $GITHUB_OUTPUT
            echo "bench_matrix={\"language\":[]}" >> $GITHUB_OUTPUT
          else
            echo "Languages to benchmark: $BENCH_LANGS"
            echo "has_benchmarks=true" >> $GITHUB_OUTPUT
            JSON_ARRAY=$(echo "$BENCH_LANGS" | tr ' ' '\n' | jq -R . | jq -s -c .)
            echo "bench_matrix={\"language\":$JSON_ARRAY}" >> $GITHUB_OUTPUT
          fi

  # Build images in parallel
  build:
    needs: prepare
    if: needs.prepare.outputs.has_builds == 'true'
    runs-on: ubicloud-standard-4
    permissions:
      contents: read
      packages: write
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.prepare.outputs.build_matrix) }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Install Dagger
        run: |
          curl -fsSL https://dl.dagger.io/dagger/install.sh | DAGGER_VERSION=${{ env.DAGGER_VERSION }} BIN_DIR=/usr/local/bin sudo -E sh

      - name: Log in to GHCR
        if: ${{ !inputs.dry_run }}
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push image
        working-directory: dagger-poc
        env:
          REGISTRY: ${{ env.REGISTRY }}
        run: |
          uv sync --quiet
          PUSH_FLAG=""
          if [ "${{ inputs.dry_run }}" != "true" ]; then
            PUSH_FLAG="--push"
          fi
          uv run dagger run python build_images.py ${{ matrix.language }} $PUSH_FLAG

  # Run benchmarks in parallel
  benchmark:
    needs: [prepare, build]
    # Run if we have benchmarks AND (builds succeeded OR no builds needed)
    if: |
      always() &&
      needs.prepare.outputs.has_benchmarks == 'true' &&
      (needs.build.result == 'success' || needs.build.result == 'skipped')
    runs-on: ubicloud-standard-4
    permissions:
      contents: read
      packages: read
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.prepare.outputs.bench_matrix) }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Install Dagger
        run: |
          curl -fsSL https://dl.dagger.io/dagger/install.sh | DAGGER_VERSION=${{ env.DAGGER_VERSION }} BIN_DIR=/usr/local/bin sudo -E sh

      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Compute cache key
        id: cache-key
        working-directory: dagger-poc
        run: |
          uv sync --quiet
          # Get the source file for this language
          SOURCE_FILE=$(uv run python -c "from languages import get_language; print(get_language('${{ matrix.language }}').file)")
          EXTRA_FILES=$(uv run python -c "from languages import get_language; l=get_language('${{ matrix.language }}'); print(' '.join([f'../src/{p}' for p in l.extra_files]))")
          IMAGE_TAG=$(uv run python -c "from build_images import get_image_tag; from languages import get_language; print(get_image_tag('${{ env.REGISTRY }}', '${{ matrix.language }}', get_language('${{ matrix.language }}')))")
          ARCH=$(uname -m)
          echo "Source file for ${{ matrix.language }}: src/$SOURCE_FILE"
          echo "Image tag for ${{ matrix.language }}: $IMAGE_TAG"

          # Compute hash of benchmark-relevant inputs.
          # Include image identity and architecture to isolate cache across toolchain/runtime changes.
          HASH=$(cat \
            "../src/$SOURCE_FILE" \
            $EXTRA_FILES \
            languages.py benchmark.py build_images.py scmeta.py detect_changes.py \
            ../src/rounds.txt ../.github/workflows/dagger-ci.yml \
            2>/dev/null | \
            { cat; echo "image_tag=$IMAGE_TAG"; echo "arch=$ARCH"; echo "dagger_version=${{ env.DAGGER_VERSION }}"; } | \
            sha256sum | cut -d' ' -f1 | head -c 16)
          echo "hash=$HASH" >> $GITHUB_OUTPUT
          echo "Cache key hash: $HASH"

      - name: Cache benchmark result
        id: cache
        if: ${{ !inputs.skip_cache && github.event_name == 'push' }}
        uses: actions/cache@v4
        with:
          path: results/${{ matrix.language }}.json
          key: dagger-benchmark-${{ matrix.language }}-${{ steps.cache-key.outputs.hash }}

      - name: Run benchmark
        if: steps.cache.outputs.cache-hit != 'true' || inputs.skip_cache
        working-directory: dagger-poc
        env:
          REGISTRY: ${{ env.REGISTRY }}
          QUICK_TEST_ROUNDS: ${{ inputs.quick_test && '10000' || '' }}
        run: |
          uv sync --quiet
          uv run dagger run python benchmark.py ${{ matrix.language }}

      - name: Upload result
        uses: actions/upload-artifact@v4
        with:
          name: result-${{ matrix.language }}
          path: results/${{ matrix.language }}.json
          if-no-files-found: warn

  # Collect results and generate analysis
  analyze:
    needs: [prepare, benchmark]
    if: always() && needs.prepare.outputs.has_benchmarks == 'true' && needs.benchmark.result == 'success'
    runs-on: ubicloud-standard-2
    steps:
      - uses: actions/checkout@v4

      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: result-*
          path: results
          merge-multiple: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: pip install matplotlib pandas

      - name: Generate summary
        run: |
          echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -d results ] && [ "$(ls results/*.json 2>/dev/null | wc -l)" -gt 0 ]; then
            echo "| Language | Median (ms) |" >> $GITHUB_STEP_SUMMARY
            echo "|:---------|------------:|" >> $GITHUB_STEP_SUMMARY
            for f in results/*.json; do
              lang=$(jq -r '.Language' "$f")
              median=$(jq -r '.Median' "$f" | awk '{printf "%.1f", $1 * 1000}')
              echo "| $lang | $median |" >> $GITHUB_STEP_SUMMARY
            done
          else
            echo "No results found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Generate analysis
        run: |
          if [ -f analyze.py ] && [ "$(ls results/*.json 2>/dev/null | wc -l)" -gt 0 ]; then
            python analyze.py --folder ./results/ --out ./results/ --rounds ./src/rounds.txt
          fi

      - name: Upload analysis
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-analysis
          path: |
            results/*.csv
            results/*.png
          if-no-files-found: ignore

  # Final summary
  summary:
    needs: [prepare, build, benchmark, analyze]
    runs-on: ubicloud-standard-2
    if: always() && github.event_name != 'issue_comment'
    steps:
      - name: Summary
        run: |
          echo "## Dagger CI Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Stage | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|:------|:-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Prepare | ${{ needs.prepare.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Build Images | ${{ needs.build.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Benchmarks | ${{ needs.benchmark.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Analysis | ${{ needs.analyze.result || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
